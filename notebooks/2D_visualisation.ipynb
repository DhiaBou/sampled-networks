{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from swimnetworks import Dense, Linear\n",
    "\n",
    "from dataset.dataset import Dataset\n",
    "from models.base_model import BaseModel\n",
    "from models.neural_net import NeuralNet\n",
    "from models.sampled_net import SampledNet\n",
    "from utils.layer1_logic import compute_weights_biases_layer1\n",
    "from view.visualizer import plot_vector_differences, plot_weight_biases_differences, plot_weight_vectors_and_point_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xd = 2  # Input space dimensions\n",
    "yd = 4  # Output dimension\n",
    "num_samples = 3000  # Number of data points\n",
    "layer_width = 200\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.create_dataset_sinus_2d(num_samples)\n",
    "dataset.scale(preprocessing.MinMaxScaler(feature_range=(-1, 1)))\n",
    "dataset.split_train_test(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "steps = [\n",
    "    (\n",
    "        \"dense\",\n",
    "        Dense(\n",
    "            layer_width=layer_width,\n",
    "            activation=\"relu\",\n",
    "            parameter_sampler=\"relu\",\n",
    "            random_seed=42,\n",
    "        ),\n",
    "    ),\n",
    "    (\"linear\", Linear(regularization_scale=1e-10)),\n",
    "]\n",
    "model_swim = Pipeline(steps)\n",
    "\n",
    "model_swim.fit(dataset.X_train, dataset.y_train)\n",
    "\n",
    "model_base = BaseModel()\n",
    "\n",
    "model_base.weights = [\n",
    "    model_swim.get_params()[\"steps\"][0][1].weights,\n",
    "    model_swim.get_params()[\"steps\"][1][1].weights,\n",
    "]\n",
    "model_base.biases = [\n",
    "    -model_swim.get_params()[\"steps\"][0][1].biases[0],\n",
    "    -model_swim.get_params()[\"steps\"][1][1].biases[0],\n",
    "]\n",
    "\n",
    "y_predict = model_base.predict(dataset.X_test)\n",
    "y_swim = model_swim.predict(dataset.X_test)\n",
    "\n",
    "print(f\"Predictions from 'model_base' and 'model_swim' are equal: {np.array_equal(y_predict, y_swim)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam = NeuralNet()\n",
    "model_adam.fit(dataset.X_train, dataset.y_train, [layer_width], epochs=10)\n",
    "\n",
    "model_base = model_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nn_train = model_base.predict(dataset.X_train)\n",
    "\n",
    "model_sampled = SampledNet()\n",
    "x_1_x2_tuples = model_sampled.fit(dataset.X_train, y_nn_train, model_base, layer2=\"classic\", radius=0,\n",
    "                                  project_onto_boundary=False)\n",
    "\n",
    "plot_weight_biases_differences(\n",
    "    model_base.weights[0], model_sampled.weights[0], model_base.biases[0], model_sampled.biases[0]\n",
    ")\n",
    "\n",
    "plot_weight_vectors_and_point_pairs(dataset.X_train, x_1_x2_tuples, np.transpose(model_base.weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xd = 2  # Input space dimensions\n",
    "num_samples = 300  # Number of data points\n",
    "layer_width = 200\n",
    "\n",
    "dataset = Dataset().create_dataset_sinus_2d(num_samples)\n",
    "dataset.scale(preprocessing.MinMaxScaler(feature_range=(-1, 1)))\n",
    "dataset.split_train_test(0.2)\n",
    "\n",
    "model = NeuralNet()\n",
    "\n",
    "initial_epochs = 20\n",
    "d_epochs = 20\n",
    "model.fit(dataset.X_train, dataset.y_train, layers=[layer_width], epochs=initial_epochs, validation_split=0.3)\n",
    "x = model.model.evaluate(dataset.X_test, dataset.y_test, verbose=0)\n",
    "\n",
    "# Initial training\n",
    "for i in range(10):\n",
    "    model.resume_training(\n",
    "        dataset.X_train,\n",
    "        dataset.y_train,\n",
    "        initial_epoch=initial_epochs,\n",
    "        epochs=initial_epochs + d_epochs,\n",
    "        validation_split=0.2,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model after additional training\n",
    "    x = model.model.evaluate(dataset.X_test, dataset.y_test, verbose=0)\n",
    "\n",
    "    print(f\"Final Validation Loss: {x:.4f}\")\n",
    "    weights_l1, biases_l1, x_1_x2_tuples = compute_weights_biases_layer1(\n",
    "        dataset.X_train, model.weights, model.biases, radius=0.0\n",
    "    )\n",
    "\n",
    "    # plot_vector_differences([weights_l1], [model.weights[0]])\n",
    "\n",
    "    plot_weight_vectors_and_point_pairs(\n",
    "        dataset.X_train, x_1_x2_tuples, np.transpose(model.weights[0]), num_vectors=20\n",
    "    )\n",
    "    plot_vector_differences([model.weights[0]], [weights_l1])\n",
    "    initial_epochs += d_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
